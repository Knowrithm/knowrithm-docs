---
title: "Data sources & ingestion"
description: "Connect documents, databases, and websites to keep Knowrithm agents fully informed."
---

## Document ingestion

<Columns cols={2}>
  <Card title="Upload API" icon="cloud-arrow-up">
    `POST /api/v1/document/upload` accepts multipart files or JSON payloads (`url`, `urls`). Provide `agent_id` to scope the content. Long-running parses return a `task_id` so you can track status.
  </Card>
  <Card title="Supported formats" icon="document-text">
    PDF, DOCX, TXT, CSV, JSON, XML, and images (via Tesseract OCR). Each document is chunked, embedded, and indexed for semantic search.
  </Card>
</Columns>

```bash
curl -X POST http://localhost:8543/api/v1/document/upload \
  -H "X-API-Key: <key>" \
  -H "X-API-Secret: <secret>" \
  -F "agent_id=<agent_id>" \
  -F "files=@playbook.pdf"
```

## Database integrations

<Tabs>
  <Tab title="Create connection">
    ```bash
    curl -X POST http://localhost:8543/api/v1/sdk/database \
      -H "X-API-Key: <key>" \
      -H "X-API-Secret: <secret>" \
      -H "Content-Type: application/json" \
      -d '{
        "name": "Production CRM",
        "database_type": "postgres",
        "url": "postgresql://bot:secret@db.internal:5432/crm",
        "agent_id": "<agent_id>"
      }'
    ```
  </Tab>
  <Tab title="Analyze schema">
    ```bash
    curl -X POST http://localhost:8543/api/v1/database-connection/<id>/analyze \
      -H "X-API-Key: <key>" \
      -H "X-API-Secret: <secret>"
    ```
    Workers reverse engineer tables, columns, relationships, and store semantic snapshots for text-to-SQL prompts.
  </Tab>
  <Tab title="Text-to-SQL">
    ```bash
    curl -X POST http://localhost:8543/api/v1/database-connection/<id>/text-to-sql \
      -H "X-API-Key: <key>" \
      -H "X-API-Secret: <secret>" \
      -H "Content-Type: application/json" \
      -d '{"question":"List the latest 10 premium customers","execute":true}'
    ```
  </Tab>
</Tabs>

<Callout icon="shield-check">
  Connection URLs are encrypted before they are persisted. Use environment-level secrets or Vault integrations to manage credentials safely.
</Callout>

## Website awareness

- Register a domain with `POST /api/v1/website/source` (supply `agent_id`, `base_url`, optional crawl policies).  
- Kick off a crawl on demand with `POST /api/v1/sdk/website/source/<source_id>/crawl` or let the widget handshake request it automatically.  
- Inspect coverage via `GET /api/v1/website/source/<source_id>/pages` to confirm when content is ready.

<Columns cols={2}>
  <Card title="Widget handshake" icon="globe-alt">
    `POST /api/v1/website/handshake` validates the hosting origin, returns the tracked page record, and can queue a crawl when the visitor loads a new URL.
  </Card>
  <Card title="Scheduling" icon="calendar-days">
    Use Celery beat to schedule routine crawls and keep embeddings in sync with fresh marketing pages or changelogs.
  </Card>
</Columns>

## Keeping sources healthy

<AccordionGroup>
  <Accordion icon="arrow-path" title="Reprocessing">
    Use `PATCH /api/v1/document/<id>/chunk/restore-all` or `PATCH /api/v1/database-connection/<id>/restore` to bring soft-deleted assets back without losing historical analytics.
  </Accordion>
  <Accordion icon="chart-bar" title="Monitor ingestion load">
    The `slow` queue handles document parsing, database analysis, and crawls. Use `GET /api/v1/analytic/usage` to monitor spikes and scale worker pools accordingly.
  </Accordion>
  <Accordion icon="trash" title="Bulk cleanup">
    `DELETE /api/v1/document/bulk-delete` and `DELETE /api/v1/database-connection/table/<table_id>` support large cleanup operations while respecting soft-delete safety nets.
  </Accordion>
</AccordionGroup>
